{"cells":[{"cell_type":"markdown","source":["# üé≠ AI-Powered Themed Star Schema Generator for Microsoft Fabric\n","\n","[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n","\n","Generate synthetic analytical datasets with Azure OpenAI and Apache Spark in Microsoft Fabric.\n","\n","---\n","\n","## üìú MIT License\n","\n","**Copyright (c) 2025 Robin Abramson**\n","\n","Permission is hereby granted, free of charge, to any person obtaining a copy\n","of this software and associated documentation files (the \"Software\"), to deal\n","in the Software without restriction, including without limitation the rights\n","to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n","copies of the Software, and to permit persons to whom the Software is\n","furnished to do so, subject to the following conditions:\n","\n","The above copyright notice and this permission notice shall be included in all\n","copies or substantial portions of the Software.\n","\n","THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n","IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n","FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n","AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n","LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n","OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n","SOFTWARE.\n","\n","---\n","\n","## üöÄ Features\n","\n","- ü§ñ **AI-Powered Content Generation**: Uses Azure OpenAI (GPT-4o) to create themed product names, locations, and business data\n","- ‚ö° **Apache Spark Processing**: Efficient large-scale data generation using PySpark\n","- üé® **Multiple Business Types**: Retail, Restaurant, Healthcare (easily extensible)\n","- üé≠ **Creative Themes**: Generate data with any theme (Space Colony, Fantasy, Cyberpunk, etc.)\n","- üîê **Secure Credentials**: Direct key configuration (remember to rotate keys after demos!)\n","- üì¶ **Fallback Mode**: Pre-loaded themes work without API access\n","- üìä **Star Schema**: Production-ready dimensional model with fact and dimension tables\n","- üé≤ **Reproducible**: Seeded random generation for consistent datasets\n","\n","---\n","\n","## üìã Prerequisites\n","\n","- **Microsoft Fabric** workspace with a Lakehouse\n","- **Azure OpenAI** deployment (optional - fallback mode available)\n","- **Fabric Capacity**: F64 or higher recommended for larger datasets\n","\n","---\n","\n","## üéØ Use Cases\n","\n","- üìä **Demo Databases**: Create engaging demo data for presentations and workshops\n","- üéì **Training**: Generate practice datasets for learning Power BI, SQL, or Spark\n","- üß™ **Testing**: Produce realistic test data for application development\n","- üé® **Prototyping**: Quickly create themed datasets for proof-of-concept work\n","\n","---\n","\n","## üèóÔ∏è Generated Schema\n","\n","This notebook creates a complete star schema:\n","\n","**Dimension Tables:**\n","- `demo_dim_date` - Complete date dimension (5 years)\n","- `demo_dim_product` - Products/services with categories and brands\n","- `demo_dim_location` - Geographic locations with regions\n","\n","**Fact Tables:**\n","- `demo_fact_sales` (Retail) or\n","- `demo_fact_orders` (Restaurant) or \n","- `demo_fact_visits` (Healthcare)\n","\n","All tables are written as **Delta Lake** format for optimal performance.\n","\n","---\n","\n","## ‚öôÔ∏è Configuration Options\n","\n","| Parameter | Options | Description |\n","|-----------|---------|-------------|\n","| `business_type` | Retail, Restaurant, Healthcare | Type of business to simulate |\n","| `theme` | Any text | Creative theme for naming (e.g., \"Space Colony\", \"Medieval Fantasy\") |\n","| `generation_mode` | AI, Preloaded | Use Azure OpenAI or pre-loaded content |\n","| `record_scale` | small, medium, large | 10K, 25K, or 500K transactions |\n","| `random_seed` | Any integer | For reproducible datasets |\n","\n","---\n","\n","## üîí Security Note\n","\n","**IMPORTANT**: This notebook includes direct API key configuration for quick demos. \n","\n","‚ö†Ô∏è **Always regenerate/rotate your Azure OpenAI key immediately after demonstrations!**\n","\n","For production use, consider:\n","- Azure Key Vault integration\n","- Managed Identity authentication\n","- Environment variables\n","\n","---\n","\n","## ü§ù Contributing\n","\n","Contributions welcome! Feel free to:\n","- Add new business types\n","- Create additional pre-loaded themes\n","- Improve data generation algorithms\n","- Enhance documentation\n","\n","---\n","\n","## üìß Contact\n","\n","Created by **Robin Abramson** - Power BI & Fabric Specialist\n","\n","---\n","\n","**Let's generate some awesome demo data! üéâ**"],"metadata":{},"id":"6b7a738f-2295-4c52-8f4d-ed6f66456b34"},{"cell_type":"markdown","source":["## üéØ Step 1: Configuration\n","\n","**Set your demo parameters here!**\n","\n","### Configuration Guide:\n","\n","- **business_type**: Choose the industry vertical\n","  - `\"Retail\"` - Products, sales, inventory\n","  - `\"Restaurant\"` - Menu items, orders, dining\n","  - `\"Healthcare\"` - Procedures, visits, treatments\n","\n","- **theme**: Any creative theme you want (e.g., \"Cyberpunk Cafe\", \"Wizarding World Retail\")\n","\n","- **generation_mode**: \n","  - `\"AI\"` - Uses Azure OpenAI to generate creative content\n","  - `\"Preloaded\"` - Uses built-in themes (no API needed)\n","\n","- **record_scale**: Dataset size\n","  - `\"small\"` - 10,000 transactions (quick demos)\n","  - `\"medium\"` - 25,000 transactions (balanced)\n","  - `\"large\"` - 500,000 transactions (stress testing)\n","\n","- **random_seed**: Set to any number for reproducible results"],"metadata":{},"id":"afd6123d-3df2-421d-a66b-e7b0ca4f371e"},{"cell_type":"code","source":["# ============================================================================\n","# CONFIGURATION\n","# ============================================================================\n","\n","# Lakehouse Configuration\n","lakehouse_name = \"\"  # Leave blank for default\n","\n","# Business Type: \"Retail\", \"Restaurant\", \"Healthcare\"\n","business_type = \"Restaurant\"\n","\n","# Theme: \"Gourmet Fantasy Food\", \"Space Colony\", etc.\n","theme = \"Caveman Fine Dining\"\n","\n","# Generation Mode: \"AI\" (use Azure OpenAI) or \"Preloaded\" (no API needed)\n","generation_mode = \"AI\"\n","\n","# Credential Source: \"direct\" \n","credential_source = \"direct\"\n","\n","# Direct credentials - if you use a direct key like this - regenerate it right away after a demo!\n","direct_endpoint = \"https://pmcai-openai.openai.azure.com/\"\n","direct_key = \"YOUR_AZURE_OPENAI_KEY_HERE\"  # ‚ö†Ô∏è ROTATE THIS KEY AFTER DEMOS!\n","direct_deployment = \"gpt4o-demo\"\n","\n","# Data Settings\n","random_seed = 42\n","record_scale = \"medium\"  # \"small\" (10K), \"medium\" (25K), \"large\" (500K)\n","\n","print(f\"üé¨ Generating {record_scale.upper()} {theme} {business_type} Dataset\")\n","print(f\"üóÑÔ∏è  Lakehouse: {lakehouse_name if lakehouse_name else 'Default'}\")\n","print(f\"ü§ñ Mode: {generation_mode}\")\n","if generation_mode == \"AI\":\n","    print(f\"üîë Credentials: Direct (from configuration)\")\n","print(f\"üé≤ Seed: {random_seed}\")\n","print(\"\\n\" + \"=\"*70)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5138c279-86f4-4741-b58a-ad320fb954a0"},{"cell_type":"markdown","source":["## üìö Step 2: Import Libraries & Setup Spark\n","\n","This cell imports all necessary libraries and initializes the Spark session.\n","\n","**What's happening:**\n","- Importing PySpark for distributed data processing\n","- Setting random seeds for reproducibility\n","- Configuring Spark for optimal performance in Fabric"],"metadata":{},"id":"e525322d-08e4-4d48-928c-83bcc80ca1e9"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql import functions as F\n","from pyspark.sql.types import *\n","import pandas as pd\n","import numpy as np\n","from datetime import datetime, timedelta\n","import random\n","import json\n","import os\n","\n","# Set random seeds for reproducibility\n","random.seed(random_seed)\n","np.random.seed(random_seed)\n","\n","# Initialize Spark session\n","spark = SparkSession.builder.getOrCreate()\n","spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")  # Optimized for Fabric\n","\n","# Set lakehouse context\n","if lakehouse_name:\n","    print(f\"üóÑÔ∏è  Using lakehouse: {lakehouse_name}\")\n","    spark.sql(f\"USE {lakehouse_name}\")\n","else:\n","    print(\"üóÑÔ∏è  Using default lakehouse\")\n","\n","print(f\"‚úÖ Spark initialized (v{spark.version})\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9e85d753-42ad-41a7-afac-3d2a0851f5af"},{"cell_type":"markdown","source":["## üîê Step 3: Load Credentials\n","\n","**‚ö†Ô∏è SECURITY WARNING**: This demo uses direct API key configuration for simplicity.\n","\n","For production scenarios, use:\n","- Azure Key Vault\n","- Managed Service Identity\n","- Environment variables\n","\n","**After any demo or presentation, immediately rotate your API keys!**"],"metadata":{},"id":"c6b4f7a1-fd9c-4b8e-9def-2e0cd1803cb2"},{"cell_type":"code","source":["azure_openai_endpoint = None\n","azure_openai_key = None\n","azure_openai_deployment = None\n","\n","if generation_mode == \"AI\":\n","    if credential_source == \"direct\":\n","        print(\"üîë Loading direct credentials\\n\")\n","        \n","        azure_openai_endpoint = direct_endpoint\n","        azure_openai_key = direct_key\n","        azure_openai_deployment = direct_deployment\n","        \n","        print(\"‚úÖ Credentials loaded successfully!\")\n","        print(f\"   Endpoint: {azure_openai_endpoint}\")\n","        print(f\"   Deployment: {azure_openai_deployment}\")\n","        print(f\"   Key: {'*' * 20} (hidden)\")\n","        \n","        # Debug info\n","        print(f\"\\nüîç Debug Info:\")\n","        print(f\"   Key length: {len(azure_openai_key)}\")\n","        print(f\"   Key starts with: {azure_openai_key[:5]}...\")\n","        \n","    else:\n","        print(\"‚ùå Key Vault not configured for this environment\")\n","        print(\"üì¶ Falling back to Preloaded mode...\")\n","        generation_mode = \"Preloaded\"\n","else:\n","    print(\"üì¶ Using Preloaded mode - no credentials needed\")\n","\n","print(\"\\n\" + \"=\"*70)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4cc477c9-8f41-4b50-8b6b-5a5cebf08ffb"},{"cell_type":"markdown","source":["## ü§ñ Step 4: AI Theme Generation\n","\n","**This is where the magic happens!**\n","\n","The notebook calls Azure OpenAI to generate:\n","- Product/menu item names with your theme\n","- Creative category names\n","- Brand names\n","- Location names\n","- Service descriptions\n","- Character names for synthetic customers\n","\n","**How it works:**\n","1. Constructs a detailed prompt based on your business type and theme\n","2. Calls Azure OpenAI API (with retry logic for reliability)\n","3. Parses the JSON response\n","4. Falls back to pre-loaded content if API fails\n","\n","**Token Management:**\n","- Uses `max_tokens: 8000` to allow for large responses\n","- Adjust if you get truncation errors\n","- Larger counts = more tokens = higher cost"],"metadata":{},"id":"08cc394a-6a0c-495b-82bf-c0e789fe060c"},{"cell_type":"code","source":["def generate_themed_content_with_azure_openai(business_type, theme):\n","    \"\"\"Use Azure OpenAI to generate creative themed content.\n","    \n","    Args:\n","        business_type: Type of business (Retail, Restaurant, Healthcare)\n","        theme: Creative theme for naming (e.g., \"Space Colony\", \"Cyberpunk\")\n","    \n","    Returns:\n","        Dictionary with themed content lists, or None if API call fails\n","    \"\"\"\n","    print(f\"ü§ñ Asking Azure OpenAI to generate {theme} content...\\n\")\n","    \n","    if not all([azure_openai_endpoint, azure_openai_key, azure_openai_deployment]):\n","        print(\"‚ö†Ô∏è  Credentials not available\")\n","        return None\n","    \n","    # Determine terminology based on business type\n","    if business_type == \"Restaurant\":\n","        product_term = \"menu items\"\n","        service_term = \"dining services\"\n","    elif business_type == \"Healthcare\":\n","        product_term = \"medical procedures and treatments\"\n","        service_term = \"medical services\"\n","    else:\n","        product_term = \"products\"\n","        service_term = \"customer services\"\n","    \n","    # Construct the prompt\n","    prompt = f\"\"\"Generate creative, themed content for a {business_type} business with a {theme} theme.\n","\n","Create these lists with UNIQUE, CREATIVE names:\n","1. product_names: 100 {product_term}\n","2. categories: 8 categories\n","3. brands: 8 brand names\n","4. locations: 50 location names\n","5. services: 6 {service_term}\n","6. adjectives: 8 descriptive adjectives\n","7. first_names: 30 character first names\n","8. last_names: 20 character last names\n","\n","Respond ONLY with valid JSON (no markdown, no explanation):\n","{{\n","  \"product_names\": [\"name1\", \"name2\", ...],\n","  \"categories\": [...],\n","  \"brands\": [...],\n","  \"locations\": [...],\n","  \"services\": [...],\n","  \"adjectives\": [...],\n","  \"first_names\": [...],\n","  \"last_names\": [...]\n","}}\"\"\"\n","    \n","    try:\n","        import requests\n","        \n","        # Try multiple API versions for compatibility\n","        api_versions = [\"2024-08-01-preview\", \"2024-06-01\", \"2024-02-15-preview\"]\n","        \n","        for api_version in api_versions:\n","            url = f\"{azure_openai_endpoint}openai/deployments/{azure_openai_deployment}/chat/completions?api-version={api_version}\"\n","            \n","            headers = {\n","                \"Content-Type\": \"application/json\",\n","                \"api-key\": azure_openai_key\n","            }\n","            \n","            data = {\n","                \"messages\": [\n","                    {\"role\": \"system\", \"content\": \"You generate creative themed content for data demos. Always respond with valid JSON only.\"},\n","                    {\"role\": \"user\", \"content\": prompt}\n","                ],\n","                \"temperature\": 0.8,  # High temperature for creativity\n","                \"max_tokens\": 8000   # Increased for larger lists\n","            }\n","            \n","            print(f\"   Trying API version: {api_version}...\")\n","            response = requests.post(url, headers=headers, json=data, timeout=30)\n","            \n","            if response.status_code == 200:\n","                content = response.json()['choices'][0]['message']['content']\n","                # Clean up potential markdown formatting\n","                content = content.replace('```json', '').replace('```', '').strip()\n","                themed_content = json.loads(content)\n","                \n","                print(\"\\n‚úÖ Azure OpenAI generated creative content!\")\n","                print(f\"   Sample products: {themed_content['product_names'][:3]}\")\n","                print(f\"   Sample brands: {themed_content['brands'][:2]}\")\n","                \n","                return themed_content\n","            elif response.status_code == 401:\n","                print(f\"   ‚ùå 401 Authentication failed\")\n","                print(f\"   Message: {response.text[:200]}\")\n","                break  # Don't try other API versions for auth errors\n","            else:\n","                print(f\"   ‚ö†Ô∏è  {response.status_code} - trying next version...\")\n","        \n","        print(f\"\\n‚ùå All API versions failed\")\n","        return None\n","        \n","    except Exception as e:\n","        print(f\"\\n‚ùå Error: {str(e)}\")\n","        return None\n","\n","\n","def get_preloaded_content(theme):\n","    \"\"\"Fallback: Use pre-generated themed content.\n","    \n","    This ensures the notebook works even without Azure OpenAI access.\n","    \"\"\"\n","    preloaded_data = {\n","        \"Gourmet Fantasy Food\": {\n","            \"product_names\": [\"Dragon's Breath Soup\", \"Moonlit Truffle Risotto\", \"Phoenix Flame Steak\", \n","                             \"Elvish Honey Cake\", \"Wizard's Whiskey Glaze\", \"Unicorn Tears Sorbet\",\n","                             \"Mermaid's Pearl Pasta\", \"Griffin Wing Roast\", \"Enchanted Forest Salad\",\n","                             \"Fairy Dust Souffl√©\", \"Troll's Mushroom Medley\", \"Celestial Berry Tart\",\n","                             \"Kraken Ink Risotto\", \"Starlight Nectar\", \"Moon-Baked Tart\",\n","                             \"Goblin's Gold Curry\", \"Siren Song Seafood\", \"Pegasus Pear Tart\", \"Basilisk Bite\", \"Chimera Chowder\"],\n","            \"categories\": [\"Mystical Appetizers\", \"Legendary Entrees\", \"Enchanted Desserts\", \n","                          \"Magical Beverages\", \"Fantasy Sides\", \"Divine Soups\", \"Ethereal Salads\", \"Mythical Mains\"],\n","            \"brands\": [\"Ivory Tower Cuisine\", \"Mystic Pantry\", \"Enchanted Eats\", \"Arcane Flavors\", \n","                      \"Celestial Kitchen\", \"Wizard's Table\", \"Dragon's Feast\", \"Elven Delights\"],\n","            \"locations\": [\"Castle Keep\", \"Mystic Grove\", \"Enchanted Garden\", \"Crystal Palace\", \n","                         \"Dragon's Lair\", \"Fairy Circle\", \"Wizard's Tower\", \"Moonlight Pavilion\", \"Starfall Inn\", \"Phoenix Nest\"],\n","            \"services\": [\"Potion Pairing\", \"Spell-Infused Cooking\", \"Mystical Wine Selection\", \"Enchantment Experience\", \"Magic Tasting Menu\", \"Alchemical Desserts\"],\n","            \"adjectives\": [\"Enchanted\", \"Mystical\", \"Legendary\", \"Ethereal\", \"Bewitched\", \"Arcane\", \"Celestial\", \"Divine\"],\n","            \"first_names\": [\"Merlin\", \"Galadriel\", \"Aragorn\", \"Luna\", \"Oberon\", \"Titania\", \"Elric\", \"Morgana\", \"Theron\", \"Selene\", \"Orion\", \"Aurora\"],\n","            \"last_names\": [\"Starweaver\", \"Moonwhisper\", \"Dragonheart\", \"Spellbinder\", \"Frostborne\", \"Shadowmere\", \"Nightshade\", \"Stormcaller\", \"Brightflame\", \"Silverwind\"]\n","        },\n","        \"Space Colony\": {\n","            \"product_names\": [\"Nebula Nutrient Pack\", \"Asteroid Mining Gear\", \"Gravity Stabilizer\", \"Oxygen Recycler Pro\",\n","                             \"Plasma Shield Generator\", \"Hyperdrive Fuel Cell\", \"Zero-G Coffee Maker\", \"Mars Habitat Module\",\n","                             \"Stellar Navigation Kit\", \"Cosmic Radiation Suit\", \"Ion Propulsion Unit\", \"Terraform Toolkit\",\n","                             \"Quantum Communicator\", \"Solar Panel Array\", \"Cryosleep Pod\", \"Antimatter Reactor\",\n","                             \"Meteor Defense System\", \"Lunar Rover Kit\", \"Warp Core\", \"Space Station Hub\"],\n","            \"categories\": [\"Life Support\", \"Mining Equipment\", \"Habitation\", \"Transportation\", \"Communication\", \"Power Systems\", \"Safety Gear\", \"Colony Infrastructure\"],\n","            \"brands\": [\"StellarTech\", \"GalaxyCorp\", \"NebulaWorks\", \"CosmicSolutions\", \"OrbitTech\", \"VoidIndustries\", \"AstroSystems\", \"Quantum Dynamics\"],\n","            \"locations\": [\"Mars Station Alpha\", \"Lunar Base Prime\", \"Asteroid Belt Outpost\", \"Jupiter Transit Hub\",\n","                         \"Saturn Ring Station\", \"Titan Colony\", \"Orbital Platform 7\", \"Europa Research Base\", \"Io Mining Station\", \"Ganymede Port\"],\n","            \"services\": [\"Gravity Adjustment\", \"Atmosphere Calibration\", \"Radiation Shielding\", \"Hypersleep Monitoring\", \"Terraforming Consultation\", \"Space Walk Training\"],\n","            \"adjectives\": [\"Advanced\", \"Cosmic\", \"Interstellar\", \"Zero-Gravity\", \"Quantum\", \"Galactic\", \"Stellar\", \"Orbital\"],\n","            \"first_names\": [\"Nova\", \"Orion\", \"Stella\", \"Cosmo\", \"Astrid\", \"Apollo\", \"Luna\", \"Atlas\", \"Vega\", \"Sirius\", \"Andromeda\", \"Phoenix\"],\n","            \"last_names\": [\"Stardust\", \"Nebula\", \"Cosmos\", \"Skywalker\", \"Astral\", \"Galaxy\", \"Void\", \"Quasar\", \"Comet\", \"Pulsar\"]\n","        }\n","    }\n","    \n","    # Return matching theme or default to Fantasy\n","    return preloaded_data.get(theme, preloaded_data[\"Gourmet Fantasy Food\"])\n","\n","\n","# Generate themed content\n","print(\"\\n\" + \"=\"*70)\n","if generation_mode == \"AI\":\n","    theme_data = generate_themed_content_with_azure_openai(business_type, theme)\n","    if theme_data is None:\n","        print(\"\\nüì¶ Falling back to preloaded content...\")\n","        theme_data = get_preloaded_content(theme)\n","else:\n","    print(\"üì¶ Using preloaded themed content\")\n","    theme_data = get_preloaded_content(theme)\n","\n","# Extract name lists for later use\n","theme_first_names = theme_data['first_names']\n","theme_last_names = theme_data['last_names']\n","\n","print(f\"\\n‚úÖ Themed content ready!\")\n","print(f\"   Products: {len(theme_data['product_names'])}\")\n","print(f\"   Categories: {len(theme_data['categories'])}\")\n","print(f\"   Locations: {len(theme_data['locations'])}\")\n","print(f\"   Brands: {len(theme_data['brands'])}\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f4df4aae-ee6f-4be7-9f62-1a4a2b31ca75"},{"cell_type":"markdown","source":["## üóìÔ∏è Step 5: Generate Date Dimension\n","\n","**Creates a complete date dimension table** with 5 years of dates (2022-2027).\n","\n","**Includes:**\n","- Basic date components (year, month, day, quarter)\n","- Day of week information\n","- Weekend flags\n","- US holiday identification\n","- Fiscal year calculations\n","\n","This is a standard dimension table used in most data warehouses."],"metadata":{},"id":"f5ddf1ef-20a1-4d85-8c3d-cf44d2e5f3aa"},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*70)\n","print(\"üìÖ Generating Date Dimension (2022-2027)\\n\")\n","\n","# Generate date range\n","start_date = datetime(2022, 1, 1)\n","end_date = datetime(2027, 12, 31)\n","date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n","\n","# US Federal Holidays for the date range\n","holidays = [\n","    \"2022-01-01\", \"2022-07-04\", \"2022-12-25\",\n","    \"2023-01-01\", \"2023-07-04\", \"2023-12-25\",\n","    \"2024-01-01\", \"2024-07-04\", \"2024-12-25\",\n","    \"2025-01-01\", \"2025-07-04\", \"2025-12-25\",\n","    \"2026-01-01\", \"2026-07-04\", \"2026-12-25\",\n","    \"2027-01-01\", \"2027-07-04\", \"2027-12-25\"\n","]\n","holidays = pd.to_datetime(holidays)\n","\n","# Build date dimension DataFrame\n","date_data = []\n","for date in date_range:\n","    date_data.append({\n","        'date_key': int(date.strftime('%Y%m%d')),\n","        'date': date,\n","        'year': date.year,\n","        'quarter': (date.month - 1) // 3 + 1,\n","        'month': date.month,\n","        'month_name': date.strftime('%B'),\n","        'day': date.day,\n","        'day_of_week': date.dayofweek + 1,  # 1=Monday, 7=Sunday\n","        'day_name': date.strftime('%A'),\n","        'week_of_year': date.isocalendar()[1],\n","        'is_weekend': int(date.dayofweek >= 5),\n","        'is_holiday': int(date in holidays),\n","        'fiscal_year': date.year if date.month >= 7 else date.year - 1\n","    })\n","\n","df_date = pd.DataFrame(date_data)\n","dim_date = spark.createDataFrame(df_date)\n","\n","print(f\"‚úÖ Generated {dim_date.count():,} dates\")\n","print(f\"   Date range: {start_date.date()} to {end_date.date()}\")\n","print(f\"   Holidays: {len(holidays)} US federal holidays included\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f8d1b47b-9dc5-4e5c-919d-cbdc4f0f18e6"},{"cell_type":"markdown","source":["## üè™ Step 6: Generate Location Dimension\n","\n","**Creates location/store dimension** using the themed location names.\n","\n","**Includes:**\n","- Themed location names\n","- Geographic hierarchy (Country ‚Üí Region ‚Üí City)\n","- Location types (Store, Outlet, Flagship, etc.)\n","- Synthetic coordinates for mapping\n","\n","For demonstrations, this creates 100 unique locations across major US cities."],"metadata":{},"id":"8b2fe7cf-e5c3-4a8f-8636-2e0a3f20e8ee"},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*70)\n","print(\"üè™ Generating Location Dimension\\n\")\n","\n","# US geographic data for realistic distribution\n","us_cities = [\n","    {\"city\": \"New York\", \"region\": \"Northeast\", \"lat\": 40.7128, \"lon\": -74.0060},\n","    {\"city\": \"Los Angeles\", \"region\": \"West\", \"lat\": 34.0522, \"lon\": -118.2437},\n","    {\"city\": \"Chicago\", \"region\": \"Midwest\", \"lat\": 41.8781, \"lon\": -87.6298},\n","    {\"city\": \"Houston\", \"region\": \"South\", \"lat\": 29.7604, \"lon\": -95.3698},\n","    {\"city\": \"Phoenix\", \"region\": \"West\", \"lat\": 33.4484, \"lon\": -112.0740},\n","    {\"city\": \"Philadelphia\", \"region\": \"Northeast\", \"lat\": 39.9526, \"lon\": -75.1652},\n","    {\"city\": \"San Antonio\", \"region\": \"South\", \"lat\": 29.4241, \"lon\": -98.4936},\n","    {\"city\": \"San Diego\", \"region\": \"West\", \"lat\": 32.7157, \"lon\": -117.1611},\n","    {\"city\": \"Dallas\", \"region\": \"South\", \"lat\": 32.7767, \"lon\": -96.7970},\n","    {\"city\": \"Austin\", \"region\": \"South\", \"lat\": 30.2672, \"lon\": -97.7431}\n","]\n","\n","location_types = [\"Flagship\", \"Standard\", \"Express\", \"Outlet\"]\n","\n","# Generate 100 locations\n","location_data = []\n","for i in range(100):\n","    city_info = random.choice(us_cities)\n","    base_name = theme_data['locations'][i % len(theme_data['locations'])]\n","    \n","    location_data.append({\n","        'location_key': i + 1,\n","        'location_name': f\"{base_name} #{i+1}\",\n","        'location_type': random.choice(location_types),\n","        'city': city_info['city'],\n","        'region': city_info['region'],\n","        'country': 'USA',\n","        'latitude': round(city_info['lat'] + random.uniform(-0.5, 0.5), 4),\n","        'longitude': round(city_info['lon'] + random.uniform(-0.5, 0.5), 4)\n","    })\n","\n","df_location = pd.DataFrame(location_data)\n","dim_location = spark.createDataFrame(df_location)\n","\n","print(f\"‚úÖ Generated {dim_location.count()} locations\")\n","print(f\"   Cities: {len(us_cities)} major US cities\")\n","print(f\"   Sample: {location_data[0]['location_name']}\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d0e5f9fb-97ec-43e2-bd02-8e6d2cb10296"},{"cell_type":"markdown","source":["## üì¶ Step 7: Generate Product Dimension\n","\n","**Creates product/service dimension** using themed names from Azure OpenAI.\n","\n","**Includes:**\n","- Themed product names\n","- Product categories\n","- Brand information\n","- Realistic pricing (varies by business type)\n","- Cost information for profit calculations\n","\n","Prices are automatically scaled based on business type:\n","- Retail: $5-$500\n","- Restaurant: $8-$150 (menu items)\n","- Healthcare: $50-$5000 (procedures/treatments)"],"metadata":{},"id":"f3b0e8df-8d4a-4e9e-8df0-5e4e1bc8f32e"},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*70)\n","print(\"üì¶ Generating Product Dimension\\n\")\n","\n","# Price ranges by business type\n","price_ranges = {\n","    \"Retail\": (5, 500),\n","    \"Restaurant\": (8, 150),\n","    \"Healthcare\": (50, 5000)\n","}\n","min_price, max_price = price_ranges.get(business_type, (10, 200))\n","\n","product_data = []\n","for i, product_name in enumerate(theme_data['product_names']):\n","    # Generate realistic pricing\n","    base_price = round(random.uniform(min_price, max_price), 2)\n","    cost = round(base_price * random.uniform(0.30, 0.60), 2)  # 30-60% cost\n","    \n","    product_data.append({\n","        'product_key': i + 1,\n","        'product_name': product_name,\n","        'category': theme_data['categories'][i % len(theme_data['categories'])],\n","        'brand': theme_data['brands'][i % len(theme_data['brands'])],\n","        'base_price': base_price,\n","        'base_cost': cost,\n","        'profit_margin': round((base_price - cost) / base_price * 100, 2)\n","    })\n","\n","df_product = pd.DataFrame(product_data)\n","dim_product = spark.createDataFrame(df_product)\n","\n","print(f\"‚úÖ Generated {dim_product.count()} products\")\n","print(f\"   Categories: {len(set(theme_data['categories']))}\")\n","print(f\"   Brands: {len(set(theme_data['brands']))}\")\n","print(f\"   Price range: ${min_price} - ${max_price}\")\n","print(f\"   Sample: {product_data[0]['product_name']} (${product_data[0]['base_price']})\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6bc8f55c-1d4c-4d7d-be86-d7fb8ac7c17e"},{"cell_type":"markdown","source":["## üí∞ Step 8: Generate Fact Table\n","\n","**Creates the main transaction fact table** with realistic business patterns.\n","\n","**What's generated:**\n","- Foreign keys to all dimension tables\n","- Transaction dates (random distribution across date range)\n","- Quantities, prices, costs\n","- Calculated measures (revenue, profit)\n","- Realistic seasonality and patterns\n","\n","**Business-specific tables:**\n","- `demo_fact_sales` for Retail\n","- `demo_fact_orders` for Restaurant\n","- `demo_fact_visits` for Healthcare\n","\n","**Performance note:** Large datasets use Spark's distributed processing for efficiency."],"metadata":{},"id":"a0a25b66-b4d8-49bc-bfeb-1c33a6e0daea"},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*70)\n","print(\"üí∞ Generating Fact Table\\n\")\n","\n","# Determine fact table name based on business type\n","fact_table_names = {\n","    \"Retail\": \"demo_fact_sales\",\n","    \"Restaurant\": \"demo_fact_orders\",\n","    \"Healthcare\": \"demo_fact_visits\"\n","}\n","fact_name = fact_table_names.get(business_type, \"demo_fact_sales\")\n","\n","# Determine number of rows based on scale\n","row_counts = {\n","    \"small\": 10_000,\n","    \"medium\": 25_000,\n","    \"large\": 500_000\n","}\n","fact_row_count = row_counts.get(record_scale, 25_000)\n","\n","print(f\"Generating {fact_row_count:,} transactions for {fact_name}...\")\n","print(\"‚è≥ This may take a moment for large datasets...\\n\")\n","\n","# Get available keys\n","date_keys = [row.date_key for row in dim_date.select('date_key').collect()]\n","location_keys = list(range(1, 101))  # 100 locations\n","product_keys = list(range(1, len(product_data) + 1))\n","\n","# Create fact records using Spark for scalability\n","@F.udf(returnType=IntegerType())\n","def random_date_key():\n","    return int(random.choice(date_keys))\n","\n","@F.udf(returnType=IntegerType())\n","def random_location_key():\n","    return random.randint(1, 100)\n","\n","@F.udf(returnType=IntegerType())\n","def random_product_key():\n","    return random.randint(1, len(product_data))\n","\n","@F.udf(returnType=IntegerType())\n","def random_quantity():\n","    return random.randint(1, 10)\n","\n","# Generate base fact table\n","fact_df = spark.range(fact_row_count) \\\n","    .withColumn(f\"{fact_name.split('_')[-1]}_key\", F.monotonically_increasing_id() + 1) \\\n","    .withColumn(\"date_key\", random_date_key()) \\\n","    .withColumn(\"location_key\", random_location_key()) \\\n","    .withColumn(\"product_key\", random_product_key()) \\\n","    .withColumn(\"quantity\", random_quantity())\n","\n","# Join with dimensions to get prices and calculate measures\n","fact_table = fact_df.join(dim_product, \"product_key\") \\\n","    .withColumn(\"unit_price\", \n","                F.round(F.col(\"base_price\") * (1 + F.rand() * 0.2 - 0.1), 2)) \\\n","    .withColumn(\"unit_cost\", \n","                F.round(F.col(\"base_cost\") * (1 + F.rand() * 0.1 - 0.05), 2)) \\\n","    .withColumn(\"discount_amount\", \n","                F.when(F.rand() < 0.15,  # 15% of transactions get discounts\n","                      F.round(F.col(\"unit_price\") * F.col(\"quantity\") * F.rand() * 0.15, 2))\n","                 .otherwise(0)) \\\n","    .withColumn(\"gross_amount\", \n","                F.round(F.col(\"unit_price\") * F.col(\"quantity\"), 2)) \\\n","    .withColumn(\"net_amount\", \n","                F.round(F.col(\"gross_amount\") - F.col(\"discount_amount\"), 2)) \\\n","    .withColumn(\"cost_amount\", \n","                F.round(F.col(\"unit_cost\") * F.col(\"quantity\"), 2)) \\\n","    .withColumn(\"profit_amount\", \n","                F.round(F.col(\"net_amount\") - F.col(\"cost_amount\"), 2))\n","\n","# Select final columns\n","key_column = f\"{fact_name.split('_')[-1]}_key\"\n","fact_table = fact_table.select(\n","    key_column,\n","    \"date_key\",\n","    \"location_key\",\n","    \"product_key\",\n","    \"quantity\",\n","    \"unit_price\",\n","    \"unit_cost\",\n","    \"gross_amount\",\n","    \"discount_amount\",\n","    F.col(\"net_amount\").alias(\"revenue\"),\n","    F.col(\"cost_amount\").alias(\"cost\"),\n","    F.col(\"profit_amount\").alias(\"profit\")\n",")\n","\n","print(f\"‚úÖ Generated {fact_table.count():,} fact records\")\n","print(f\"   Table: {fact_name}\")\n","print(f\"   Date range: Full 2022-2027 with realistic distribution\")\n","print(f\"   Locations: 100 stores\")\n","print(f\"   Products: {len(product_data)}\")\n","\n","# Show sample statistics\n","stats = fact_table.select(\n","    F.sum(\"revenue\").alias(\"total_revenue\"),\n","    F.sum(\"profit\").alias(\"total_profit\"),\n","    F.avg(\"revenue\").alias(\"avg_transaction\")\n",").collect()[0]\n","\n","print(f\"\\nüìä Quick Stats:\")\n","print(f\"   Total Revenue: ${stats.total_revenue:,.2f}\")\n","print(f\"   Total Profit: ${stats.total_profit:,.2f}\")\n","print(f\"   Avg Transaction: ${stats.avg_transaction:,.2f}\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"53c3a23a-d0a2-4f06-ac48-0dcfb1008f38"},{"cell_type":"markdown","source":["## üíæ Step 9: Write Tables to Lakehouse\n","\n","**Saves all tables to Delta Lake format** in your Fabric Lakehouse.\n","\n","**What happens:**\n","1. Drops any existing demo tables (clean slate)\n","2. Writes all 4 tables (date, location, product, fact)\n","3. Uses Delta Lake format for:\n","   - ACID transactions\n","   - Time travel\n","   - Optimized storage\n","   - Power BI Direct Lake compatibility\n","\n","**Tables created:**\n","- `demo_dim_date`\n","- `demo_dim_location`\n","- `demo_dim_product`\n","- `demo_fact_sales` / `demo_fact_orders` / `demo_fact_visits`\n","\n","These tables immediately appear in:\n","- Lakehouse SQL endpoint\n","- Power BI datasets (Direct Lake)\n","- Fabric notebooks"],"metadata":{},"id":"a65e4e5f-c4e9-496a-8896-30bafed10f8d"},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*70)\n","print(\"üíæ Writing Tables to Lakehouse...\\n\")\n","\n","# Drop existing demo tables if they exist\n","tables_to_drop = [\"demo_dim_date\", \"demo_dim_location\", \"demo_dim_product\", \n","                  \"demo_fact_sales\", \"demo_fact_orders\", \"demo_fact_visits\"]\n","\n","for table in tables_to_drop:\n","    try:\n","        spark.sql(f\"DROP TABLE IF EXISTS {table}\")\n","    except:\n","        pass  # Table doesn't exist, continue\n","\n","# Write dimension tables\n","print(\"Writing dimension tables...\")\n","\n","print(\"   Writing demo_dim_date...\")\n","dim_date.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"demo_dim_date\")\n","print(f\"      ‚úÖ {dim_date.count():,} rows\\n\")\n","\n","print(\"   Writing demo_dim_location...\")\n","dim_location.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"demo_dim_location\")\n","print(f\"      ‚úÖ {dim_location.count():,} rows\\n\")\n","\n","print(\"   Writing demo_dim_product...\")\n","dim_product.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"demo_dim_product\")\n","print(f\"      ‚úÖ {dim_product.count():,} rows\\n\")\n","\n","# Write fact table\n","print(f\"Writing fact table: {fact_name}...\")\n","fact_table.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(fact_name)\n","print(f\"      ‚úÖ {fact_table.count():,} rows\\n\")\n","\n","print(\"=\"*70)\n","print(\"‚úÖ All tables written successfully!\")\n","print(\"\\nüìä Tables are now available in:\")\n","print(\"   - Lakehouse Tables folder\")\n","print(\"   - SQL Analytics Endpoint\")\n","print(\"   - Power BI Direct Lake mode\")\n","print(\"\\nüí° Next: Open Power BI to create visualizations!\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"aea8b9d2-df77-49ce-ac19-ede13d1008af"},{"cell_type":"markdown","source":["## üîÑ Optional: Refresh Table Cache\n","\n","**Run this cell after generating new data** to clear Spark's cache.\n","\n","This ensures that:\n","- SQL queries see the latest data\n","- Power BI gets fresh results\n","- Fabric semantic models update properly\n","\n","**Note:** This cell dynamically finds all demo tables, so it works regardless of which business type you generated!"],"metadata":{},"id":"961d03f5-1008-af25-4639-821e-d695ba5b8b88"},{"cell_type":"code","source":["# ============================================================================\n","# üîÑ REFRESH ALL TABLES (Dynamic - works for any business type)\n","# ============================================================================\n","\n","print(\"üîÑ Refreshing all demo tables...\")\n","print(\"=\"*70 + \"\\n\")\n","\n","# Get all tables in the current database\n","tables = spark.sql(\"SHOW TABLES\").select(\"tableName\").rdd.flatMap(lambda x: x).collect()\n","\n","# Refresh all demo tables that exist\n","refreshed = 0\n","for table in tables:\n","    if table.startswith(\"demo_\"):\n","        try:\n","            spark.sql(f\"REFRESH TABLE {table}\")\n","            print(f\"   ‚úÖ Refreshed: {table}\")\n","            refreshed += 1\n","        except Exception as e:\n","            print(f\"   ‚ö†Ô∏è  Couldn't refresh {table}: {str(e)}\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(f\"‚úÖ Refreshed {refreshed} tables - cache cleared!\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bc32aab1-b0bc-4cef-81a7-26f9d7e16e00"},{"cell_type":"markdown","source":["## üìà Step 10: Sample Analytics\n","\n","**Quick analytics to verify your data looks good!**\n","\n","This cell demonstrates:\n","- Joining fact and dimension tables\n","- Aggregating metrics (revenue, profit, transactions)\n","- Finding top performers\n","\n","**You can use these queries as templates** for your own analysis or Power BI reports!"],"metadata":{},"id":"f02bdcfb-8f04-46f0-819c-204af14bf41d"},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*70)\n","print(\"üìà SAMPLE ANALYTICS\\n\")\n","\n","print(\"üí∞ Daily Revenue (Last 10 Days):\")\n","daily_revenue = fact_table.join(dim_date, \"date_key\") \\\n","    .groupBy(\"date\") \\\n","    .agg(\n","        F.sum(\"revenue\").alias(\"revenue\"),\n","        F.sum(\"profit\").alias(\"profit\"),\n","        F.count(\"*\").alias(\"transactions\")\n","    ).orderBy(F.desc(\"date\")).limit(10)\n","daily_revenue.show(truncate=False)\n","\n","print(\"\\nüèÜ Top 10 Products by Revenue:\")\n","top_products = fact_table.join(dim_product, \"product_key\") \\\n","    .groupBy(\"product_name\", \"category\") \\\n","    .agg(\n","        F.sum(\"revenue\").alias(\"revenue\"),\n","        F.sum(\"quantity\").alias(\"units\")\n","    ).orderBy(F.desc(\"revenue\")).limit(10)\n","top_products.show(truncate=False)\n","\n","print(\"\\nüè™ Top 10 Locations:\")\n","location_perf = fact_table.join(dim_location, \"location_key\") \\\n","    .groupBy(\"location_name\", \"city\") \\\n","    .agg(\n","        F.sum(\"revenue\").alias(\"revenue\"),\n","        F.count(\"*\").alias(\"transactions\")\n","    ).orderBy(F.desc(\"revenue\")).limit(10)\n","location_perf.show(truncate=False)\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"‚úÖ Generation Complete!\")\n","print(f\"\\nüéâ Your {theme} {business_type} dataset is ready!\")\n","print(f\"   - {fact_row_count:,} transactions generated\")\n","print(f\"   - 4 tables written to lakehouse\")\n","print(f\"   - Ready for analytics, ML, and BI!\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6f318f10-8ead-43f2-8bff-86c020d1d572"},{"cell_type":"markdown","source":["## üéì Next Steps & Usage Ideas\n","\n","### üéØ What to do with your data:\n","\n","**1. üìä Build Power BI Dashboards**\n","- Open Power BI Desktop\n","- Connect to your Lakehouse (Direct Lake mode)\n","- Create stunning visualizations with your themed data!\n","- Your audience will love the creative names\n","\n","**2. üíª Run SQL Queries**\n","\n","Try these sample queries in the SQL Analytics Endpoint:\n","\n","```sql\n","-- Revenue by Category\n","SELECT \n","    p.category,\n","    SUM(f.revenue) as total_revenue,\n","    COUNT(*) as transactions,\n","    AVG(f.revenue) as avg_transaction\n","FROM demo_fact_orders f  -- Change to your fact table name\n","JOIN demo_dim_product p ON f.product_key = p.product_key\n","GROUP BY p.category\n","ORDER BY total_revenue DESC;\n","\n","-- Monthly Trends\n","SELECT \n","    d.year,\n","    d.month_name,\n","    SUM(f.revenue) as revenue,\n","    SUM(f.profit) as profit\n","FROM demo_fact_orders f\n","JOIN demo_dim_date d ON f.date_key = d.date_key\n","GROUP BY d.year, d.month, d.month_name\n","ORDER BY d.year, d.month;\n","\n","-- Top Performing Locations\n","SELECT \n","    l.location_name,\n","    l.city,\n","    l.region,\n","    SUM(f.revenue) as total_revenue,\n","    COUNT(*) as transactions\n","FROM demo_fact_orders f\n","JOIN demo_dim_location l ON f.location_key = l.location_key\n","GROUP BY l.location_name, l.city, l.region\n","ORDER BY total_revenue DESC\n","LIMIT 20;\n","```\n","\n","**3. ü§ñ Train Machine Learning Models**\n","- Revenue forecasting with Prophet or ARIMA\n","- Customer segmentation clustering\n","- Product recommendation engines\n","- Anomaly detection on transaction patterns\n","\n","**4. üé® Try Different Themes!**\n","\n","Fun theme ideas:\n","- \"Cyberpunk Street Food\" (Restaurant)\n","- \"Underwater Boutique\" (Retail)\n","- \"Time Travel Clinic\" (Healthcare)\n","- \"Dinosaur Theme Park\" (Retail)\n","- \"Steampunk Workshop\" (Retail)\n","- \"Alien Embassy\" (Healthcare)\n","\n","**5. üìà Scale Up**\n","- Change `record_scale` to \"large\" for 500K transactions\n","- Test Power BI performance with larger datasets\n","- Benchmark your Spark queries\n","\n","**6. üé¨ Create Demos & Presentations**\n","- The themed data makes presentations more engaging\n","- Audiences remember \"Dragon's Breath Soup\" better than \"Product_042\"\n","- Perfect for conference talks and workshops\n","\n","---\n","\n","## üîß Troubleshooting\n","\n","**Problem: \"Unterminated string\" error**\n","- Solution: Increase `max_tokens` in Step 4 (try 8000 or 16000)\n","\n","**Problem: Authentication failed (401)**\n","- Check your Azure OpenAI endpoint URL\n","- Verify your API key is correct\n","- Ensure your deployment name matches\n","- Remember to regenerate keys after demos!\n","\n","**Problem: Table not found when refreshing**\n","- Use the dynamic refresh cell provided\n","- It auto-detects your fact table name\n","\n","**Problem: Out of memory**\n","- Reduce `record_scale` to \"small\" or \"medium\"\n","- Increase your Fabric capacity if needed\n","\n","---\n","\n","## üìö Resources\n","\n","- [Microsoft Fabric Documentation](https://learn.microsoft.com/fabric/)\n","- [Power BI Direct Lake](https://learn.microsoft.com/power-bi/enterprise/directlake-overview)\n","- [Azure OpenAI Service](https://learn.microsoft.com/azure/ai-services/openai/)\n","- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)\n","\n","---\n","\n","## üéâ Share Your Creations!\n","\n","Created something cool with this notebook? Share it!\n","- Post screenshots of your themed dashboards\n","- Share creative theme ideas\n","- Contribute improvements on GitHub\n","\n","---\n","\n","**Happy analyzing! üöÄ**\n","\n","*Remember: Always regenerate your Azure OpenAI keys after demonstrations!*"],"metadata":{},"id":"f00565eb-f31b-412f-bf2d-b6cc075133e9"}],"metadata":{"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"kernel_info":{"name":"synapse_pyspark"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"bc1637b7-f40a-4bc9-8b8e-d941ba1c84ec"}],"default_lakehouse":"bc1637b7-f40a-4bc9-8b8e-d941ba1c84ec","default_lakehouse_name":"SQLSaturdayLake","default_lakehouse_workspace_id":"26bb60a8-d9cb-4dfe-a4e4-02093143fde7"}}},"nbformat":4,"nbformat_minor":5}
